{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08d7810",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.0.0\n",
      "  Downloading tensorflow-2.0.0-cp36-cp36m-win_amd64.whl (48.1 MB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting tensorboard<2.1.0,>=2.0.0\n",
      "  Downloading tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
      "  Downloading tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.15.0-cp36-cp36m-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\robcog\\anaconda3\\envs\\intents\\lib\\site-packages (from tensorflow==2.0.0) (0.37.1)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.48.2-cp36-cp36m-win_amd64.whl (3.6 MB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting protobuf>=3.6.1\n",
      "  Downloading protobuf-3.19.6-cp36-cp36m-win_amd64.whl (897 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\robcog\\anaconda3\\envs\\intents\\lib\\site-packages (from tensorflow==2.0.0) (1.16.0)\n",
      "Collecting numpy<2.0,>=1.16.0\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.1.0-cp36-cp36m-win_amd64.whl (2.7 MB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\robcog\\anaconda3\\envs\\intents\\lib\\site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (58.0.4)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting typing-extensions>=3.6.4\n",
      "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\robcog\\anaconda3\\envs\\intents\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2021.5.30)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Building wheels for collected packages: gast, termcolor\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=6084baae44b09e80383cfb9ec1e5fe1405c7d89c1a1d166b42ca69191029c38a\n",
      "  Stored in directory: c:\\users\\robcog\\appdata\\local\\pip\\cache\\wheels\\19\\a7\\b9\\0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=c6a28f7e69a4008a1c9df95fda4621bd9164fbaf3274b9799aa1c028273061a4\n",
      "  Stored in directory: c:\\users\\robcog\\appdata\\local\\pip\\cache\\wheels\\93\\2a\\eb\\e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built gast termcolor\n",
      "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, zipp, typing-extensions, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, numpy, importlib-metadata, google-auth, dataclasses, cached-property, werkzeug, protobuf, markdown, h5py, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras-applications, google-pasta, gast, astor, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astor-0.8.1 cached-property-1.5.2 cachetools-4.2.4 charset-normalizer-2.0.12 dataclasses-0.8 gast-0.2.2 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.48.2 h5py-3.1.0 idna-3.4 importlib-metadata-4.8.3 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.7 numpy-1.19.5 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-2.27.1 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1 termcolor-1.1.0 typing-extensions-4.1.1 urllib3-1.26.16 werkzeug-2.0.3 wrapt-1.15.0 zipp-3.6.0\n",
      "Requirement already satisfied: grpcio in c:\\users\\robcog\\anaconda3\\envs\\intents\\lib\\site-packages (1.48.2)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\robcog\\anaconda3\\envs\\intents\\lib\\site-packages (from grpcio) (1.16.0)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\robcog\\anaconda3\\envs\\intents\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\robcog\\anaconda3\\envs\\intents\\lib\\site-packages (from importlib-resources->tqdm) (3.6.0)\n",
      "Installing collected packages: importlib-resources, tqdm\n",
      "Successfully installed importlib-resources-5.4.0 tqdm-4.64.1\n",
      "Collecting bert-for-tf2\n",
      "  Using cached bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
      "Collecting py-params>=0.9.6\n",
      "  Using cached py-params-0.10.2.tar.gz (7.4 kB)\n",
      "Collecting params-flow>=0.8.0\n",
      "  Using cached params-flow-0.8.2.tar.gz (22 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\robcog\\anaconda3\\envs\\intents\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\robcog\\anaconda3\\envs\\intents\\lib\\site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.64.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\robcog\\anaconda3\\envs\\intents\\lib\\site-packages (from tqdm->params-flow>=0.8.0->bert-for-tf2) (5.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\robcog\\anaconda3\\envs\\intents\\lib\\site-packages (from tqdm->params-flow>=0.8.0->bert-for-tf2) (0.4.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\robcog\\anaconda3\\envs\\intents\\lib\\site-packages (from importlib-resources->tqdm->params-flow>=0.8.0->bert-for-tf2) (3.6.0)\n",
      "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
      "  Building wheel for bert-for-tf2 (setup.py): started\n",
      "  Building wheel for bert-for-tf2 (setup.py): finished with status 'done'\n",
      "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30535 sha256=2a4271230612f3d2f379682a2cd759969cc12c66bf94bf2804537ed19746030c\n",
      "  Stored in directory: c:\\users\\robcog\\appdata\\local\\pip\\cache\\wheels\\19\\18\\97\\0d58cab1b354f98e6b1b3791e9b2dc14ce537d63daec0d4924\n",
      "  Building wheel for params-flow (setup.py): started\n",
      "  Building wheel for params-flow (setup.py): finished with status 'done'\n",
      "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19472 sha256=0b8dd70ba1e9748fe5a5f82d5b24e7f68db3c562b1a935691257f25a447f7870\n",
      "  Stored in directory: c:\\users\\robcog\\appdata\\local\\pip\\cache\\wheels\\9d\\c6\\c5\\a3f4a618640f461130f47e76703156c13a39cc3b96b5f4f439\n",
      "  Building wheel for py-params (setup.py): started\n",
      "  Building wheel for py-params (setup.py): finished with status 'done'\n",
      "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7911 sha256=de2cc70d76cbfe7c34e2b463faffa9d8235871b4677da2f119e7b1368720c74d\n",
      "  Stored in directory: c:\\users\\robcog\\appdata\\local\\pip\\cache\\wheels\\48\\77\\b1\\d0b01d524d972512852e1fc23b197ad9d3d1685e349b8cabb6\n",
      "Successfully built bert-for-tf2 params-flow py-params\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
      "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp36-cp36m-win_amd64.whl (990 kB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.0.0\n",
    "!pip install --upgrade grpcio\n",
    "!pip install tqdm\n",
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece\n",
    "# !pip install tensorflow-gpu\n",
    "# !pip install wget\n",
    "# !pip install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aba24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install seaborn\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18967198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import rc\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e955f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Data/train.csv\")\n",
    "valid = pd.read_csv(\"Data/valid.csv\")\n",
    "test = pd.read_csv(\"Data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "198db79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat((train,valid),axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d99cbe4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13779</th>\n",
       "      <td>is any cinema playing the spirit of youth</td>\n",
       "      <td>SearchScreeningEvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13780</th>\n",
       "      <td>what are the movie times for animated movies i...</td>\n",
       "      <td>SearchScreeningEvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13781</th>\n",
       "      <td>what s the movie schedule at great escape thea...</td>\n",
       "      <td>SearchScreeningEvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13782</th>\n",
       "      <td>show the times for cheers for miss bishop at d...</td>\n",
       "      <td>SearchScreeningEvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13783</th>\n",
       "      <td>i want to see married to the enemy 2 at a cinema</td>\n",
       "      <td>SearchScreeningEvent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text                intent\n",
       "13779          is any cinema playing the spirit of youth  SearchScreeningEvent\n",
       "13780  what are the movie times for animated movies i...  SearchScreeningEvent\n",
       "13781  what s the movie schedule at great escape thea...  SearchScreeningEvent\n",
       "13782  show the times for cheers for miss bishop at d...  SearchScreeningEvent\n",
       "13783   i want to see married to the enemy 2 at a cinema  SearchScreeningEvent"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd208b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name=\"uncased_L-12_H-768_A-12\"\n",
    "bert_ckpt_dir = os.path.join(\"model/\", bert_model_name)\n",
    "bert_ckpt_file = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n",
    "bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88e88c",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35c6aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentDetectionData:\n",
    "  DATA_COLUMN = \"text\"\n",
    "  LABEL_COLUMN = \"intent\"\n",
    "\n",
    "  def __init__(self, train, test, tokenizer: FullTokenizer, classes, max_seq_len=192):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_seq_len = 0\n",
    "    self.classes = classes\n",
    "    \n",
    "    ((self.train_x, self.train_y), (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
    "\n",
    "    print(\"max seq_len\", self.max_seq_len)\n",
    "    self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "    self.train_x, self.test_x = map(self._pad, [self.train_x, self.test_x])\n",
    "\n",
    "  def _prepare(self, df):\n",
    "    x, y = [], []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "      text, label = row[IntentDetectionData.DATA_COLUMN], row[IntentDetectionData.LABEL_COLUMN]\n",
    "      tokens = self.tokenizer.tokenize(text)\n",
    "      tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "      token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "      self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "      x.append(token_ids)\n",
    "      y.append(self.classes.index(label))\n",
    "\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "  def _pad(self, ids):\n",
    "    x = []\n",
    "    for input_ids in ids:\n",
    "      input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "      input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "      x.append(np.array(input_ids))\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f67811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a47f7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'can', \"'\", 't', 'wait', 'to', 'visit', 'bulgaria', 'again', '!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"I can't wait to visit Bulgaria again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f83ac31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1045, 2064, 1005, 1056, 3524, 2000, 3942, 8063, 2153, 999]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"I can't wait to visit Bulgaria again!\")\n",
    "tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf30dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_seq_len, bert_ckpt_file):\n",
    "    \n",
    "    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "        bc = StockBertConfig.from_json_string(reader.read())\n",
    "        bert_params = map_stock_config_to_params(bc)\n",
    "        bert_params.adapter_size = None\n",
    "        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "        \n",
    "    input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name=\"input_ids\")\n",
    "    bert_output = bert(input_ids)\n",
    "    \n",
    "    print(\"bert shape\", bert_output.shape)\n",
    "    \n",
    "    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n",
    "    cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "    logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "    logits = keras.layers.Dropout(0.5)(logits)\n",
    "    logits = keras.layers.Dense(units=len(classes), activation=\"softmax\")(logits)\n",
    "    \n",
    "    model = keras.Model(inputs=input_ids, outputs=logits)\n",
    "    model.build(input_shape=(None, max_seq_len))\n",
    "\n",
    "    load_stock_weights(bert, bert_ckpt_file)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6299a536",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e8ae161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13784it [00:02, 5073.70it/s]\n",
      "C:\\Users\\robcog\\anaconda3\\envs\\Intents\\lib\\site-packages\\ipykernel_launcher.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "700it [00:00, 5013.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 38\n"
     ]
    }
   ],
   "source": [
    "classes = train.intent.unique().tolist()\n",
    "\n",
    "data = IntentDetectionData(train, test, tokenizer, classes, max_seq_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "637f7da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13784, 38)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfb1e4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  4952,  2000,  2225,  3676,  2213,  2632, 25438, 27395,\n",
       "        2006,  8224,  2189,   102,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6be1db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cecc91b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2236ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 38, 768)\n",
      "Done loading 196 BERT weights from: model/uncased_L-12_H-768_A-12\\bert_model.ckpt into <bert.model.BertModelLayer object at 0x000001C598AE4B70> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n"
     ]
    }
   ],
   "source": [
    "model = create_model(data.max_seq_len, bert_ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9d79514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 38)]              0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 38, 768)           108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 5383      \n",
      "=================================================================\n",
      "Total params: 109,486,087\n",
      "Trainable params: 109,486,087\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0c7d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=keras.optimizers.Adam(1e-5),\n",
    "  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377a95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12405 samples, validate on 1379 samples\n",
      "Epoch 1/5\n",
      " 3920/12405 [========>.....................] - ETA: 52:11 - loss: 1.5293 - acc: 0.6745"
     ]
    }
   ],
   "source": [
    "# log_dir = \"log/intent_detection/\"\n",
    "# tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "history = model.fit(\n",
    "  x=data.train_x, \n",
    "  y=data.train_y,\n",
    "  validation_split=0.1,\n",
    "  batch_size=16,\n",
    "  shuffle=True,\n",
    "  epochs=5,\n",
    "#   callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d8ffdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
